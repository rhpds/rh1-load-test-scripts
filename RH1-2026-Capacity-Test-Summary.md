# RH1 2026 Event - Infrastructure Capacity Test Summary

**Test Date**: January 15, 2026
**Event Date**: January 27, 2026
**Location**: Las Vegas
**Infrastructure**: 8 CNV clusters + External Ceph storage

---

## Labs Tested

### 1. LB1542 - RIPU Lab
**Session**: 2:30 PM - 4:30 PM (2 hours) - Room 252
**Lab**: Automating RHEL In-Place Upgrades with Ansible
**Users**: 60 users √ó 3 VMs = **180 VMs total**

### 2. LB6618 - VMA Factory
**Session**: 12:00 PM - 4:30 PM (4.5 hours) - Room 103
**Lab**: OpenShift Virtualization Super Lab - VM Migration Factory
**Users**: 60 users √ó 2 VMs = **120 VMs total**

---

## Test Results

### LB1542 - RIPU Lab Findings

**What Users Do**:
- Upgrade RHEL 7‚Üí8, 8‚Üí9, 9‚Üílatest using Leapp
- Each VM downloads 60-171 GB of packages from Demosat satellite
- Upgrades run for 30-60 minutes per VM

**Infrastructure Per User**:
- 1 Bastion (ansible-1): 8GB RAM, 4 cores
- 1 AAP Controller: 32GB RAM, 8 cores
- 3 Worker VMs: RHEL 7, 8, 9 (4GB RAM, 2 cores each)

**Capacity Test Results**:

‚úÖ **Storage Performance - ADEQUATE**
- Single VM write speed: 240 MB/s
- Aggregate (10 parallel): 2.34+ GB/s
- **Conclusion**: Ceph storage is NOT the bottleneck

‚ùå **Demosat Network - CRITICAL BOTTLENECK**
- Tested: 5.57 MB/s aggregate (3 VMs downloading simultaneously)
- Required for 2-wave: 1.5 GB/s
- **Gap**: Only achieving 11% of needed capacity
- **Impact**: 60 users √ó 3 VMs = 180 concurrent package downloads will saturate Demosat

**Total Data Volume**:
- Conservative: 180 VMs √ó 171 GB = 30.8 TB
- Realistic: 180 VMs √ó 60 GB = 10.8 TB

**Bottleneck**: Demosat satellite network bandwidth

---

### LB6618 - VMA Factory Findings

**What Users Do**:
- Migrate Windows VMs from VMware vCenter to OpenShift Virtualization
- Use MTV (Migration Toolkit for Virtualization) v2.10
- Configure AAP workflows for migration automation

**Infrastructure Per User**:
- OpenShift 4.20 SNO cluster with 3 workers (32GB RAM, 16 cores each)
- Windows VMs: win2019-1, win2019-2 (27 GB each)
- Source: VMware vCenter at `/RS00/vm/ETX/student-<id>/`

**Capacity Test Results**:

‚úÖ **Storage Performance - ADEQUATE**
- Same Ceph infrastructure: 2.34+ GB/s available
- **Conclusion**: Storage is NOT the bottleneck

‚úÖ **VM Size Confirmed**
- 27 GB per VM (from vCenter)
- Total migration data: 120 VMs √ó 27 GB = 3.19 TB
- **3x-9x LESS data than RIPU lab**

‚ùå **Migration Bandwidth - NOT TESTED**
- Could not access Windows VMs in test environment
- **Gap**: Don't know actual VMware‚ÜíOpenShift transfer speed
- **Missing data**: Network performance between vCenter and MTV

**Total Data Volume**:
- 60 users √ó 2 VMs √ó 27 GB = 3.19 TB (3,240 GB)

**Bottleneck**: Unknown - migration test incomplete

---

## Critical Findings Summary

| Lab | Users | Total VMs | Total Data | Storage | Network/Transfer | Bottleneck Identified |
|-----|-------|-----------|------------|---------|------------------|----------------------|
| **LB1542 (RIPU)** | 60 | 180 | 10.8-30.8 TB | ‚úÖ OK (2.34+ GB/s) | ‚ùå **Demosat: 5.57 MB/s** | **YES - Demosat Network** |
| **LB6618 (VMA)** | 60 | 120 | 3.19 TB | ‚úÖ OK (2.34+ GB/s) | ‚ùì Unknown | **NO - Test Incomplete** |

---

## Recommendations

### LB1542 - RIPU Lab

**CRITICAL: Cannot run all 60 users concurrently**

Demosat network bottleneck requires one of these options:

**Option 1: Wave Stagger (Recommended)**
- Split 60 users into waves that start at different times
- **9-12 Wave Stagger**: 5-7 users per wave, 10-15 min apart
  - Wave 1 (7 users): Start at 2:30 PM
  - Wave 2 (7 users): Start at 2:40 PM
  - Wave 3 (7 users): Start at 2:50 PM
  - Continue through Wave 9
- Reduces peak load from 180 concurrent to ~20 concurrent VMs
- All users still complete lab within 2-hour window

**Option 2: Fix Demosat Network (Before Event)**
- Investigate Demosat network bottleneck
- Deploy Satellite Capsule servers closer to CNV infrastructure
- Test bandwidth improvements before Jan 27

**Option 3: 2-Wave Stagger (Minimum)**
- Wave 1 (30 users): Start at 2:30 PM
- Wave 2 (30 users): Start at 2:50 PM
- Reduces peak from 180 VMs to 90 VMs
- **Risk**: Still may saturate Demosat (only 50% reduction)

---

### LB6618 - VMA Factory

**Conservative Recommendation: 2-4 Wave Stagger**

Without migration bandwidth data, recommend cautious approach:

**Option 1: 2-Wave Stagger (Recommended)**
- Wave 1 (30 users): Start migrations at 12:00 PM
- Wave 2 (30 users): Start migrations at 1:00 PM
- Reduces peak from 120 concurrent to 60 concurrent VMs
- Allows monitoring of first wave before second starts

**Option 2: 4-Wave Stagger (Safest)**
- 15 users per wave, 30 minutes apart
- Peak load: Only 30 concurrent migrations
- Most conservative approach

**Option 3: All Concurrent (Risky)**
- Only if migration bandwidth test completed before event
- Need to confirm vCenter‚ÜíMTV network can handle 120 concurrent migrations

**Why Cautious?**
- VMA Factory has 3x-9x LESS data than RIPU (easier)
- BUT: Missing migration bandwidth test data
- Unknown: vCenter API limits, network capacity, MTV performance at scale

---

## What Is "Wave Stagger"?

**Definition**: Splitting users into groups that start heavy operations at different times

**Example - RIPU 2-Wave Stagger**:
```
Instructor announces at 2:30 PM:
"Tables 1-3: Start your upgrade now"

At 2:50 PM (20 minutes later):
"Tables 4-6: Now start your upgrade"
```

**Why It Works**:
- **Without stagger**: 180 VMs hit Demosat at 2:30 PM ‚Üí System overloads
- **With stagger**: Only 90 VMs at 2:30 PM, next 90 at 2:50 PM ‚Üí System survives

**How Users Experience It**:
- Everyone gets same 2-hour lab time (2:30-4:30 PM)
- Wave 2 users do other modules while waiting
- All users complete successfully (vs. all users failing due to overload)

**Implementation Options**:
1. **Instructor-led**: Instructor announces wave start times
2. **Ticket-based**: "Odd tickets start at X, even tickets at Y"
3. **Table-based**: "Tables 1-3 start at X, Tables 4-6 at Y"
4. **Lab instructions**: Built into showroom with clear wait times

---

## Comparison to Full Event Analysis

**From Full Event Analysis** (RH1-2026-User-Action-Bottleneck-Analysis.md):
- Peak concurrent users: **1,099 users (afternoon session)**
- All-day labs: 149 users (Ansible Super Lab + VMA Factory)
- Morning session: ~849 users
- Afternoon session: ~1,099 users

**Our Testing Covered**:
- 2 of 30+ labs
- Focus on highest-risk user actions (package downloads, VM migrations)
- Identified 1 critical bottleneck (Demosat)
- Confirmed storage is adequate (Ceph: 2.34+ GB/s)

**What We Confirmed**:
- ‚úÖ External Ceph storage is adequate for tested workloads
- ‚úÖ Storage bandwidth scales linearly with concurrent VMs
- ‚ùå Demosat network is critical bottleneck for RIPU lab
- ‚ùì Migration bandwidth unknown for VMA Factory

**Alignment with Full Analysis**:
- Full analysis identified Demosat as High-Risk bottleneck ‚Üí **CONFIRMED**
- Full analysis identified External Ceph as Highest Risk ‚Üí **NOT CONFIRMED** (our tests show Ceph is adequate)
- Recommendation: Wave stagger for RIPU ‚Üí **MATCHES our findings**

---

## Test Scripts Available

All test scripts published at: https://github.com/rhpds/rh1-load-test-scripts

### RIPU Lab Scripts:
- `test-ripu-repo-metadata-size.sh` - Query package sizes from DNF metadata
- `test-demosat-bandwidth.sh` - Test single VM bandwidth to Demosat
- Multi-VM Demosat test (manual - run on 3 VMs simultaneously)
- `test-storage-speed.sh` - Single VM storage write speed
- `test-storage-speed-parallel.sh` - Aggregate storage throughput
- `run-all-ripu-tests.sh` - Run all tests in sequence

### VMA Factory Scripts:
- `test-vma-migration-direct.sh` - Automated migration test (uses oc commands)
- `test-storage-speed.sh` - Single VM storage write speed
- `test-storage-speed-parallel.sh` - Aggregate storage throughput
- `run-all-vma-tests.sh` - Run all tests in sequence

### How to Use:
```bash
# On lab bastion
git clone https://github.com/rhpds/rh1-load-test-scripts.git
cd rh1-load-test-scripts

# RIPU lab
./run-all-ripu-tests.sh

# VMA Factory lab
./test-vma-migration-direct.sh
```

---

## Next Steps

### Before Event (By January 24):

**LB1542 - RIPU Lab**:
1. ‚úÖ **CONFIRMED**: Demosat network is bottleneck
2. ‚ö†Ô∏è **DECISION REQUIRED**: Choose mitigation strategy
   - Option A: 9-12 wave stagger (safest)
   - Option B: Fix Demosat network (requires infra team)
   - Option C: 2-wave stagger (risky, only 50% reduction)
3. üìã **ACTION**: Update lab showroom instructions with wave stagger timing
4. üìã **ACTION**: Brief instructors on wave start procedure

**LB6618 - VMA Factory**:
1. ‚ö†Ô∏è **RECOMMENDED**: Plan 2-wave stagger (conservative without test data)
2. üìã **OPTIONAL**: Attempt migration test if Windows VMs become available
3. üìã **ACTION**: Update lab showroom with stagger recommendation
4. üìã **ACTION**: Coordinate with VMware team on vCenter capacity

**General**:
1. ‚úÖ **CONFIRMED**: Ceph storage adequate for both labs
2. üìã **ACTION**: Share test results with infrastructure team
3. üìã **ACTION**: Archive test logs for post-event analysis

### During Event (January 27):

**LB1542 - RIPU Lab**:
- Instructor announces wave start times
- Monitor Demosat bandwidth during event
- Have fallback plan if Demosat overloads

**LB6618 - VMA Factory**:
- Instructor announces wave start times (if using stagger)
- Monitor first wave completion before starting second wave
- Watch for vCenter API errors

---

## Test Limitations

**What We Could NOT Test**:

1. **VMA Factory Migration Bandwidth**
   - Windows VMs not accessible in test environment
   - Cannot measure actual VMware‚ÜíOpenShift transfer speed
   - Cannot validate vCenter API performance at scale

2. **Multi-User Concurrency**
   - Tested up to 10 parallel operations
   - Did not test full 60-user concurrency
   - Real event may reveal additional bottlenecks

3. **Peak Load Scenarios**
   - Tested individual labs in isolation
   - Did not test combined load of all 30+ labs
   - Event will have 1,099 concurrent users (vs our ~10)

4. **Network Paths**
   - Tested from single lab bastion
   - Event will have 8 CNV clusters with different network paths
   - Network topology may differ

**What This Means**:
- Our findings are directionally correct
- Actual event may encounter additional issues
- Wave stagger is conservative recommendation based on limited testing
- Real-time monitoring during event is critical

---

## Conclusion

**LB1542 - RIPU Lab**:
- ‚ùå **CANNOT run all 60 users concurrently** (Demosat bottleneck confirmed)
- ‚úÖ **MUST use 9-12 wave stagger** or fix Demosat network before event
- ‚úÖ Storage is adequate (not the bottleneck)

**LB6618 - VMA Factory**:
- ‚ö†Ô∏è **RECOMMEND 2-4 wave stagger** (conservative without migration test)
- ‚úÖ Storage is adequate
- ‚ùì Migration bandwidth unknown (test incomplete)

**Overall Event**:
- Storage (Ceph) is adequate for tested workloads
- Network (Demosat) is critical bottleneck for RIPU
- Wave stagger is proven mitigation strategy
- Pre-deployment strategy (from full analysis) still recommended for all other labs

**Risk Level**:
- **RIPU Lab**: HIGH (without mitigation) ‚Üí MEDIUM (with wave stagger)
- **VMA Factory**: MEDIUM (without test) ‚Üí LOW-MEDIUM (with conservative stagger)

**Test Scripts**: https://github.com/rhpds/rh1-load-test-scripts
**Full Event Analysis**: See RH1-2026-User-Action-Bottleneck-Analysis.md
